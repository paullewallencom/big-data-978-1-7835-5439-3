{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n# ** Basic Introduction to a few Spark Commands **\n\nThis notebook is based on tutorials conducted by [Databricks](https://databricks.com). The tutorial will be conducted using the Databricks' Community Edition of Spark available for sign up [here](https://databricks.com/try-databricks). Databricks is a leading provider of the commercial and enterprise supported version of Spark.\n\nIn this lab, we will introduce a few basic commands used in Spark. Users are encouraged to try out more extensive Spark tutorials and notebooks that are available on the web for more detailed examples.\n\nDocumentation for [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.sql)."],"metadata":{}},{"cell_type":"code","source":["# The SparkContext/SparkSession is the entry point for all Spark operations\n# sc = the SparkContext = the execution environment of Spark, only 1 per JVM\n# Note that SparkSession is now the entry point (from Spark v2.0)\n# This tutorial uses SparkContext (was used prior to Spark 2.0)\n\nfrom pyspark import SparkContext\n# sc = SparkContext(appName = \"some_application_name\") # You'd normally run this, but in this case, it has already been created in the Databricks' environment"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["quote = \"To be, or not to be, that is the question:  Whether 'tis nobler in the mind to suffer  The slings and arrows of outrageous fortune,  Or to take Arms against a Sea of troubles,  And by opposing end them: to die, to sleep  No more; and by a sleep, to say we end  the heart-ache, and the thousand natural shocks  that Flesh is heir to? 'Tis a consummation  devoutly to be wished. To die, to sleep,  To sleep, perchance to Dream; aye, there's the rub,  for in that sleep of death, what dreams may come,  when we have shuffled off this mortal coil, must give us pause.\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["sparkdata = sc.parallelize(quote.split(' '))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["print \"sparkdata = \", sparkdata\nprint \"sparkdata.collect = \", sparkdata.collect\nprint \"sparkdata.collect() = \", sparkdata.collect()[1:10]"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# A simple transformation - map\n\ndef mapword(word):\n  return (word,1)\n\nprint sparkdata.map(mapword) # Nothing has happened here\nprint sparkdata.map(mapword).collect()[1:10] # collect causes the DAG to execute"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Another Transformation\n\ndef charsmorethan2(tuple1):\n  if len(tuple1[0])>2:\n    return tuple1\n  pass\n\nrdd3 = sparkdata.map(mapword).filter(lambda x: charsmorethan2(x))\n# Multiple Transformations in 1 statement, nothing is happening yet\n\nrdd3.collect()[1:10] # The DAG gets executed. Note that since we didn't remove punctuation marks ... 'be,', etc are also included"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# With Tables, a general example\n\ncms = sc.parallelize([[1,\"Dr. A\",12.50,\"Yale\"],[2,\"Dr. B\",5.10,\"Duke\"],[3,\"Dr. C\",200.34,\"Mt. Sinai\"],[4,\"Dr. D\",5.67,\"Duke\"],[1,\"Dr. E\",52.50,\"Yale\"]])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["def findPayment(data):\n  return data[2]\n\nprint \"Payments = \", cms.map(findPayment).collect()\nprint \"Mean = \", cms.map(findPayment).mean() # Mean is an action"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Creating a DataFrame (familiar to Python programmers)\n\ncms_df = sqlContext.createDataFrame(cms, [\"ID\",\"Name\",\"Payment\",\"Hosp\"])\nprint cms_df.show()\nprint cms_df.groupby('Hosp').agg(func.avg('Payment'), func.max('Payment'),func.min('Payment'))\nprint cms_df.groupby('Hosp').agg(func.avg('Payment'), func.max('Payment'),func.min('Payment')).collect()\nprint\nprint \"Converting to a Pandas DataFrame\"\nprint \"--------------------------------\"\npd_df = cms_df.groupby('Hosp').agg(func.avg('Payment'), func.max('Payment'),func.min('Payment')).toPandas()\nprint type(pd_df)\nprint\nprint pd_df\n\n\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["wordsList = ['to','be','or','not','to','be']\nwordsRDD = sc.parallelize(wordsList, 3) # Splits into 2 groups\n# Print out the type of wordsRDD\nprint type(wordsRDD)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Glom coallesces all elements within each partition into a list\nprint wordsRDD.glom().take(2) # Take is an action, here we are 'take'-ing the first 2 elements of the wordsRDD\nprint wordsRDD.glom().collect() # Collect"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# An example with changing the case of words\n\n# One way of completing the function\ndef makeUpperCase(word):\n    return word.upper()\n\nprint makeUpperCase('cat')\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["upperRDD = wordsRDD.map(makeUpperCase)\nprint upperRDD.collect()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["upperLambdaRDD = wordsRDD.map(lambda word: word.upper())\nprint upperLambdaRDD.collect()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Pair RDDs\nwordPairs = wordsRDD.map(lambda word: (word, 1))\nprint wordPairs.collect()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["#### Part 2: Counting with pair RDDs \nThere are multiple ways of performing group-by operations in Spark\nOne such method is groupByKey()\n\n** Using groupByKey() **\n\nThis method creates a key-value pair whereby each key (in this case word) is assigned a value of 1 for our wordcount operation. It then combines all keys into a single list. This can be quite memory intensive, especially if the dataset is large."],"metadata":{}},{"cell_type":"code","source":["# Using groupByKey\nwordsGrouped = wordPairs.groupByKey()\nfor key, value in wordsGrouped.collect():\n    print '{0}: {1}'.format(key, list(value))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Summation of the key values (to get the word count)\nwordCountsGrouped = wordsGrouped.map(lambda (k,v): (k, sum(v)))\nprint wordCountsGrouped.collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["** (2c) Counting using reduceByKey **\n\nreduceByKey creates a new pair RDD. It then iteratively applies a function first to each key (i.e., within the key values) and then across all the keys, i.e., in other words it applies the given function iteratively."],"metadata":{}},{"cell_type":"code","source":["wordCounts = wordPairs.reduceByKey(lambda a,b: a+b)\nprint wordCounts.collect()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["** Combining all of the above into a single statement **"],"metadata":{}},{"cell_type":"code","source":["wordCountsCollected = (wordsRDD\n                       .map(lambda word: (word, 1))\n                       .reduceByKey(lambda a,b: a+b)\n                       .collect())\nprint wordCountsCollected"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["This tutorial has provided a basic overview of Spark and introduced the Databricks community edition where users can upload and execute their own Spark notebooks. There are various in-depth tutorials on the web and also at Databricks on Spark and users are encouraged to peruse them if interested in learning further about Spark."],"metadata":{}}],"metadata":{"name":"Packt_Notebook","notebookId":538254486733003},"nbformat":4,"nbformat_minor":0}
